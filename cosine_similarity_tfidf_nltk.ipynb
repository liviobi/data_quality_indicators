{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lbiondo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lbiondo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "# used for looping through folders/files\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "#Calc tfidf and cosine similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All text entries to compare will appear here\n",
    "BASE_INPUT_DIR = \"./data/\"\n",
    "INPUT_DIR = \"./data/input/input_file.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnListOfFilePaths(inputPath,folderPath):\n",
    "    fileInfo = []\n",
    "    listOfFileNames = [fileName for fileName in listdir(folderPath) if isfile(join(folderPath, fileName))]\n",
    "    listOfFilePaths = [join(folderPath, fileName) for fileName in listdir(folderPath) if isfile(join(folderPath, fileName))]\n",
    "    \n",
    "    listOfFileNames.append(\"input_file\")\n",
    "    listOfFilePaths.append(inputPath)\n",
    "    \n",
    "    fileInfo.append(listOfFileNames)\n",
    "    fileInfo.append(listOfFilePaths)\n",
    "    return fileInfo\n",
    "\n",
    "fileNames, filePaths = returnListOfFilePaths(INPUT_DIR,BASE_INPUT_DIR)\n",
    "#print(fileNames, \"\\n\", filePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document contents\n",
    "def create_docContentDict(filePaths):\n",
    "    rawContentDict = {}\n",
    "    for filePath in filePaths:\n",
    "        with open(filePath, \"r\") as ifile:\n",
    "            fileContent = ifile.read()\n",
    "        rawContentDict[filePath] = fileContent\n",
    "    return rawContentDict\n",
    "rawContentDict = create_docContentDict(filePaths)\n",
    "#print(rawContentDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to use within the tokenizer\n",
    "We'd like to;\n",
    "- tokenize the input\n",
    "- remove stop words\n",
    "- perform stemming\n",
    "- remove punctuation\n",
    "- convert input to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeContent(contentsRaw):\n",
    "    tokenized = nltk.tokenize.word_tokenize(contentsRaw)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWordsFromTokenized(contentsTokenized):\n",
    "    stop_word_set = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    filteredContents = [word for word in contentsTokenized if word not in stop_word_set]\n",
    "    return filteredContents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performPorterStemmingOnContents(contentsTokenized):\n",
    "    porterStemmer = nltk.stem.PorterStemmer()\n",
    "    filteredContents = [porterStemmer.stem(word) for word in contentsTokenized]\n",
    "    return filteredContents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuationFromTokenized(contentsTokenized):\n",
    "    excludePuncuation = set(string.punctuation)\n",
    "    \n",
    "    # manually add additional punctuation to remove\n",
    "    doubleSingleQuote = '\\'\\''\n",
    "    doubleDash = '--'\n",
    "    doubleTick = '``'\n",
    "\n",
    "    excludePuncuation.add(doubleSingleQuote)\n",
    "    excludePuncuation.add(doubleDash)\n",
    "    excludePuncuation.add(doubleTick)\n",
    "\n",
    "    filteredContents = [word for word in contentsTokenized if word not in excludePuncuation]\n",
    "    return filteredContents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert terms to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertItemsToLower(contentsRaw):\n",
    "    filteredContents = [term.lower() for term in contentsRaw]\n",
    "    return filteredContents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test that functions are working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".START \n",
      "\n",
      "Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.\n",
      "Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get contents of a file for testing\n",
    "# TODO: may need to make a copy of this here\n",
    "content_test = rawContentDict[filePaths[0]]\n",
    "\n",
    "# visually inspect\n",
    "print(content_test[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.START', 'Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.', 'Mr.', 'Vinken', 'is', 'chairman', 'of', 'Elsevier', 'N.V.', ',', 'the', 'Dutch', 'publishing']\n"
     ]
    }
   ],
   "source": [
    "# test tokenization\n",
    "content_test_tokenized = tokenizeContent(content_test)\n",
    "\n",
    "# visually inspect\n",
    "print(content_test_tokenized[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.START', 'Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'join', 'board', 'nonexecutive', 'director', 'Nov.', '29', '.', 'Mr.', 'Vinken', 'chairman', 'Elsevier', 'N.V.', ',', 'Dutch', 'publishing', 'group', '.']\n"
     ]
    }
   ],
   "source": [
    "# test remove stop words\n",
    "content_test_rmStop = removeStopWordsFromTokenized(content_test_tokenized)\n",
    "\n",
    "# visually inspect\n",
    "print(content_test_rmStop[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.start', 'pierr', 'vinken', ',', '61', 'year', 'old', ',', 'join', 'board', 'nonexecut', 'director', 'nov.', '29', '.', 'mr.', 'vinken', 'chairman', 'elsevi', 'n.v.', ',', 'dutch', 'publish', 'group', '.']\n"
     ]
    }
   ],
   "source": [
    "# Test stemming\n",
    "content_test_stemmed = performPorterStemmingOnContents(content_test_rmStop)\n",
    "\n",
    "# visually inspect\n",
    "print(content_test_stemmed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.start', 'pierr', 'vinken', '61', 'year', 'old', 'join', 'board', 'nonexecut', 'director', 'nov.', '29', 'mr.', 'vinken', 'chairman', 'elsevi', 'n.v.', 'dutch', 'publish', 'group']\n"
     ]
    }
   ],
   "source": [
    "# Test remove punctuation\n",
    "content_test_cleaned = removePunctuationFromTokenized(content_test_stemmed)\n",
    "\n",
    "# visually inspect\n",
    "print(content_test_cleaned[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.start', 'pierr', 'vinken', '61', 'year', 'old', 'join', 'board', 'nonexecut', 'director', 'nov.', '29', 'mr.', 'vinken', 'chairman', 'elsevi', 'n.v.', 'dutch', 'publish', 'group']\n"
     ]
    }
   ],
   "source": [
    "# Test convert to lower\n",
    "content_test_clean_lower = convertItemsToLower(content_test_cleaned)\n",
    "print(content_test_clean_lower[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap into a function to be used by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data without writing inspection file information to file\n",
    "def processData(rawContents):\n",
    "    cleaned = tokenizeContent(rawContents)\n",
    "    cleaned = removeStopWordsFromTokenized(cleaned)\n",
    "    cleaned = performPorterStemmingOnContents(cleaned)    \n",
    "    cleaned = removePunctuationFromTokenized(cleaned)\n",
    "    cleaned = convertItemsToLower(cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Functions For Output\n",
    "- TFIDF\n",
    "- Cosine Similarity\n",
    "    - this function will both calcuate and output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print TFIDF values in 'table' format\n",
    "def print_TFIDF_for_all(term, values, fileNames):\n",
    "    values = values.transpose() # files along 'x-axis', terms along 'y-axis'\n",
    "    numValues = len(values[0])\n",
    "    print('                ', end=\"\")   #bank space for formatting output\n",
    "    for n in range(len(fileNames)):\n",
    "        print('{0:18}'.format(fileNames[n]), end=\"\")    #file names\n",
    "    print()\n",
    "    for i in range(len(term)):\n",
    "        print('{0:8}'.format(term[i]), end='\\t|  ')     #the term\n",
    "        for j in range(numValues):\n",
    "            print('{0:.12f}'.format(values[i][j]), end='   ') #the value, corresponding to the file name, for the term\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: modify this to build matrix then print from matrix form\n",
    "def calc_and_print_CosineSimilarity_for_all(tfs, fileNames):\n",
    "    #print(cosine_similarity(tfs[0], tfs[1]))\n",
    "    print(\"\\n\\n\\n========COSINE SIMILARITY====================================================================\\n\")\n",
    "    numFiles = len(fileNames)\n",
    "    for i in range(numFiles -1):\n",
    "        print(fileNames[i], end='   ')\n",
    "        matrixValue = cosine_similarity(tfs[i], tfs[-1])\n",
    "        numValue = matrixValue[0][0]\n",
    "        print(\" {0:.8f}\".format(numValue), end='         ')\n",
    "\n",
    "        print()\n",
    "    print(\"\\n\\n=============================================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(printResults=True):\n",
    "    baseFolderPath = \"./data/\"\n",
    "    inputFilePath = \"./data/input/input_file.txt\"\n",
    "\n",
    "    fileNames, filePathList = returnListOfFilePaths(inputFilePath,baseFolderPath)\n",
    "\n",
    "    rawContentDict = create_docContentDict(filePathList)\n",
    "\n",
    "    # instanciate tfid\n",
    "    tfidf = TfidfVectorizer(tokenizer=processData, stop_words='english')\n",
    "    #calculate tfidf\n",
    "    tfs = tfidf.fit_transform(rawContentDict.values())\n",
    "    tfs_Values = tfs.toarray()\n",
    "    tfs_Term = tfidf.get_feature_names()\n",
    "    \n",
    "    # print results\n",
    "    #print_TFIDF_for_all(tfs_Term, tfs_Values, fileNames)\n",
    "    calc_and_print_CosineSimilarity_for_all(tfs, fileNames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "========COSINE SIMILARITY====================================================================\n",
      "\n",
      "wsj_0001    1.00000000         \n",
      "wsj_0002    0.15622490         \n",
      "wsj_0003    0.00949398         \n",
      "wsj_0004    0.00322659         \n",
      "wsj_0005    0.04396144         \n",
      "wsj_0006    0.00278221         \n",
      "wsj_0007    0.00181271         \n",
      "wsj_0008    0.01661047         \n",
      "wsj_0009    0.03139419         \n",
      "wsj_0010    0.02208738         \n",
      "wsj_0011    0.01584017         \n",
      "wsj_0012    0.01441098         \n",
      "wsj_0013    0.00660417         \n",
      "wsj_0014    0.06737963         \n",
      "wsj_0015    0.00984402         \n",
      "wsj_0016    0.00860567         \n",
      "wsj_0017    0.00183360         \n",
      "wsj_0018    0.01168064         \n",
      "wsj_0019    0.02164322         \n",
      "wsj_0020    0.00047055         \n",
      "wsj_0021    0.01565625         \n",
      "wsj_0022    0.01082278         \n",
      "wsj_0023    0.00147117         \n",
      "wsj_0024    0.00478892         \n",
      "wsj_0025    0.02371089         \n",
      "wsj_0026    0.00491147         \n",
      "wsj_0027    0.02638177         \n",
      "wsj_0028    0.11371539         \n",
      "wsj_0029    0.01560089         \n",
      "wsj_0030    0.06271723         \n",
      "wsj_0031    0.00222195         \n",
      "wsj_0032    0.06958331         \n",
      "wsj_0033    0.00167924         \n",
      "wsj_0034    0.01371073         \n",
      "wsj_0035    0.00286006         \n",
      "wsj_0036    0.01342688         \n",
      "wsj_0037    0.03318934         \n",
      "wsj_0038    0.03191044         \n",
      "wsj_0039    0.02474413         \n",
      "wsj_0040    0.00126291         \n",
      "wsj_0041    0.03368212         \n",
      "wsj_0042    0.02012733         \n",
      "wsj_0043    0.01166367         \n",
      "wsj_0044    0.01091163         \n",
      "wsj_0045    0.01985768         \n",
      "wsj_0046    0.00204817         \n",
      "wsj_0047    0.01081919         \n",
      "wsj_0048    0.01594207         \n",
      "wsj_0049    0.02404142         \n",
      "wsj_0050    0.00208943         \n",
      "wsj_0051    0.00628044         \n",
      "wsj_0052    0.01088873         \n",
      "wsj_0053    0.01364901         \n",
      "wsj_0054    0.00174649         \n",
      "wsj_0055    0.00297592         \n",
      "wsj_0056    0.00254210         \n",
      "wsj_0057    0.00800227         \n",
      "wsj_0058    0.06479659         \n",
      "wsj_0059    0.00463580         \n",
      "wsj_0060    0.00605939         \n",
      "wsj_0061    0.01274663         \n",
      "wsj_0062    0.02269627         \n",
      "wsj_0063    0.00441104         \n",
      "wsj_0064    0.00566827         \n",
      "wsj_0065    0.00635609         \n",
      "wsj_0066    0.10796447         \n",
      "wsj_0067    0.02190666         \n",
      "wsj_0068    0.00168671         \n",
      "wsj_0069    0.12762335         \n",
      "wsj_0070    0.06358951         \n",
      "wsj_0071    0.01079684         \n",
      "wsj_0072    0.01141165         \n",
      "wsj_0073    0.02409877         \n",
      "wsj_0074    0.00153865         \n",
      "wsj_0075    0.02040667         \n",
      "wsj_0076    0.00219372         \n",
      "wsj_0077    0.01001257         \n",
      "wsj_0078    0.01160166         \n",
      "wsj_0079    0.00207870         \n",
      "wsj_0080    0.02195194         \n",
      "wsj_0081    0.00083562         \n",
      "wsj_0082    0.01455445         \n",
      "wsj_0083    0.02147096         \n",
      "wsj_0084    0.04515345         \n",
      "wsj_0085    0.01078153         \n",
      "wsj_0086    0.00164977         \n",
      "wsj_0087    0.00314340         \n",
      "wsj_0088    0.02967420         \n",
      "wsj_0089    0.01445622         \n",
      "wsj_0090    0.01581952         \n",
      "wsj_0091    0.00076160         \n",
      "wsj_0092    0.00635944         \n",
      "wsj_0093    0.02570749         \n",
      "wsj_0094    0.00096700         \n",
      "wsj_0095    0.01064506         \n",
      "wsj_0096    0.01266910         \n",
      "wsj_0097    0.01807593         \n",
      "wsj_0098    0.00828635         \n",
      "wsj_0099    0.00141611         \n",
      "wsj_0100    0.03713684         \n",
      "wsj_0101    0.00724904         \n",
      "wsj_0102    0.01801064         \n",
      "wsj_0103    0.00610718         \n",
      "wsj_0104    0.02673679         \n",
      "wsj_0105    0.00698104         \n",
      "wsj_0106    0.00163588         \n",
      "wsj_0107    0.00430808         \n",
      "wsj_0108    0.00712836         \n",
      "wsj_0109    0.05819342         \n",
      "wsj_0110    0.00297808         \n",
      "wsj_0111    0.07626362         \n",
      "wsj_0112    0.00244566         \n",
      "wsj_0113    0.01465108         \n",
      "wsj_0114    0.01900565         \n",
      "wsj_0115    0.00192421         \n",
      "wsj_0116    0.01540523         \n",
      "wsj_0117    0.00045517         \n",
      "wsj_0118    0.03468930         \n",
      "wsj_0119    0.00540042         \n",
      "wsj_0120    0.00551897         \n",
      "wsj_0121    0.00681232         \n",
      "wsj_0122    0.00194682         \n",
      "wsj_0123    0.00652420         \n",
      "wsj_0124    0.03725331         \n",
      "wsj_0125    0.02746963         \n",
      "wsj_0126    0.00588790         \n",
      "wsj_0127    0.02076994         \n",
      "wsj_0128    0.00884024         \n",
      "wsj_0129    0.02869529         \n",
      "wsj_0130    0.00126556         \n",
      "wsj_0131    0.00589274         \n",
      "wsj_0132    0.06267995         \n",
      "wsj_0133    0.00771690         \n",
      "wsj_0134    0.00120447         \n",
      "wsj_0135    0.01002175         \n",
      "wsj_0136    0.00397245         \n",
      "wsj_0137    0.01254792         \n",
      "wsj_0138    0.00089227         \n",
      "wsj_0139    0.00961595         \n",
      "wsj_0140    0.02212744         \n",
      "wsj_0141    0.01857352         \n",
      "wsj_0142    0.00975404         \n",
      "wsj_0143    0.00881507         \n",
      "wsj_0144    0.02017196         \n",
      "wsj_0145    0.00095700         \n",
      "wsj_0146    0.01494691         \n",
      "wsj_0147    0.04423069         \n",
      "wsj_0148    0.02490267         \n",
      "wsj_0149    0.02602330         \n",
      "wsj_0150    0.00108356         \n",
      "wsj_0151    0.02960081         \n",
      "wsj_0152    0.04656591         \n",
      "wsj_0153    0.02154050         \n",
      "wsj_0154    0.00113407         \n",
      "wsj_0155    0.01255676         \n",
      "wsj_0156    0.00094396         \n",
      "wsj_0157    0.01186885         \n",
      "wsj_0158    0.06306196         \n",
      "wsj_0159    0.02656127         \n",
      "wsj_0160    0.00490304         \n",
      "wsj_0161    0.01218983         \n",
      "wsj_0162    0.02564566         \n",
      "wsj_0163    0.00105230         \n",
      "wsj_0164    0.00551696         \n",
      "wsj_0165    0.00307095         \n",
      "wsj_0166    0.02812708         \n",
      "wsj_0167    0.00129872         \n",
      "wsj_0168    0.00784890         \n",
      "wsj_0169    0.01162946         \n",
      "wsj_0170    0.00288537         \n",
      "wsj_0171    0.00725092         \n",
      "wsj_0172    0.00710541         \n",
      "wsj_0173    0.00111259         \n",
      "wsj_0174    0.00589963         \n",
      "wsj_0175    0.00111672         \n",
      "wsj_0176    0.00109893         \n",
      "wsj_0177    0.00530702         \n",
      "wsj_0178    0.03211451         \n",
      "wsj_0179    0.01363772         \n",
      "wsj_0180    0.00073586         \n",
      "wsj_0181    0.02148375         \n",
      "wsj_0182    0.00901115         \n",
      "wsj_0183    0.00290068         \n",
      "wsj_0184    0.01403053         \n",
      "wsj_0185    0.07088400         \n",
      "wsj_0186    0.01614180         \n",
      "wsj_0187    0.00059315         \n",
      "wsj_0188    0.01922391         \n",
      "wsj_0189    0.01395154         \n",
      "wsj_0190    0.00164186         \n",
      "wsj_0191    0.00353719         \n",
      "wsj_0192    0.00893458         \n",
      "wsj_0193    0.00190598         \n",
      "wsj_0194    0.01193801         \n",
      "wsj_0195    0.00346757         \n",
      "wsj_0196    0.04595269         \n",
      "wsj_0197    0.01589645         \n",
      "wsj_0198    0.00810077         \n",
      "wsj_0199    0.00913019         \n",
      "\n",
      "\n",
      "=============================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20cb975c02109accbafa2be35d047791792b4e263bc20b6c63105d80068546e3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tesi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

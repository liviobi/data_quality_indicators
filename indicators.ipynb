{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bef5d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lbiondo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\lbiondo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import enchant\n",
    "from spello.model import SpellCorrectionModel\n",
    "import textstat\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d30344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lbiondo\\Anaconda3\\envs\\tesi\\lib\\site-packages\\spello\\model.py:301: UserWarning: This model was saved on spell<1.3.0. As such due to a bug in previous versions, none of customisations made to the config at the time of training were saved along with the model. It is recommended to load the model, apply all required customizations to config and save it again. E.g.\n",
      "\n",
      "from spello.model import SpellCorrectionModel \n",
      "sp = SpellCorrectionModel(language='en')  \n",
      "sp.load('/home/ubuntu/model.pkl')\n",
      "sp.config.min_length_for_spellcorrection = 4 # default is 3\n",
      "sp.config.max_length_for_spellcorrection = 12 # default is 15\n",
      "sp.save(model_save_dir='/home/ubuntu/')\n",
      "\n",
      "After this the model will load without any warnings\n",
      "\n",
      "  warnings.warn(\"This model was saved on spell<1.3.0. As such due to a bug in previous versions, \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spello.model.SpellCorrectionModel at 0x1814fd9bf10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load spell checker model\n",
    "sp = SpellCorrectionModel(language='en')\n",
    "sp.load('./spello_model/en_large.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42370b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuationFromTokenized(contentsTokenized):\n",
    "    excludePuncuation = set(string.punctuation)\n",
    "    \n",
    "    # manually add additional punctuation to remove\n",
    "    doubleSingleQuote = '\\'\\''\n",
    "    doubleDash = '--'\n",
    "    doubleTick = '``'\n",
    "\n",
    "    excludePuncuation.add(doubleSingleQuote)\n",
    "    excludePuncuation.add(doubleDash)\n",
    "    excludePuncuation.add(doubleTick)\n",
    "\n",
    "    filteredContents = [word for word in contentsTokenized if word not in excludePuncuation]\n",
    "    return filteredContents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae9d0e6",
   "metadata": {},
   "source": [
    "#### Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "881834bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "   return (len(set(text)) / len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14926723",
   "metadata": {},
   "source": [
    "#### Percentage of Uppercased and Lowercased Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e3004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_uppercased(text_tokenized):\n",
    "    islower = 0;\n",
    "    isupper = 0;\n",
    "    total_words = len(text_tokenized);\n",
    "\n",
    "    for word in text_tokenized:      \n",
    "        if word.islower():\n",
    "            islower +=1         \n",
    "        elif word.isupper():\n",
    "            isupper+=1         \n",
    "\n",
    "    return (islower/total_words,isupper/total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bfec8",
   "metadata": {},
   "source": [
    "#### Spell Checking Against Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fe900f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_checker_1(text_tokenized):\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    mistakes = 0\n",
    "    for word in text_tokenized:\n",
    "        if not d.check(word):\n",
    "            mistakes+=1\n",
    "    return mistakes        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df6df67",
   "metadata": {},
   "source": [
    "#### Spell Checking with ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "986728bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_checker_2(raw_text,text_tokenized):\n",
    "    corrected = sp.spell_correct(raw_text)\n",
    "    i = 0\n",
    "    for w in text_tokenized:\n",
    "        if(w in corrected['correction_dict']):\n",
    "            i+=1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d6848",
   "metadata": {},
   "source": [
    "#### Unrecognized Words by POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "632e3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unrecognized_by_pos(text_tokenized):\n",
    "    text_tagged = nltk.pos_tag(text_tokenized,tagset='universal')\n",
    "    unknown = 0\n",
    "    for t in text_tagged:\n",
    "         if t[1] == \"X\":\n",
    "                #return t\n",
    "                unknown += 1\n",
    "    return unknown/len(text_tagged)            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c628c",
   "metadata": {},
   "source": [
    "#### Average Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8752ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest\n",
    "import re\n",
    "\n",
    "\n",
    "def avg_sentence_length(text):\n",
    "    \n",
    "    terminating_punct = \"[!?.]\"\n",
    "    punct = r\"\\W\"  # non-word characters\n",
    "    sentences = [\n",
    "        s.strip()  # without trailing whitespace\n",
    "        for s in re.split(\n",
    "            terminating_punct,\n",
    "            \"\".join(text).replace(\"\\n\", \" \"),  # text as 1 string\n",
    "        )\n",
    "        if s.strip()  # non-empty\n",
    "    ]\n",
    "\n",
    "    def wordcount(s):\n",
    "        \"\"\"Split sentence s on punctuation\n",
    "        and return number of non-empty words\n",
    "        \"\"\"\n",
    "        return len([w for w in re.split(punct, s) if w])\n",
    "    #map each sentece to its wordcount then sum all the wordcounts\n",
    "    return sum(map(wordcount, sentences)) / len(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb2c0ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical diversity: 0.426\n",
      "Lowercase sentences: 0.925\n",
      "Uppercase sentences: 0.001\n",
      "Spelling mistakes 1: 23\n",
      "Spelling mistakes 2: 32\n",
      "Unrecognized by POS tagger: 0.0\n",
      "Average sentence length: 16.172\n",
      "Readability (CLI): 8.13\n",
      "Readability (ARI): 11.8\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/input/prose.txt\"\n",
    "\n",
    "with open(path, \"r\") as ifile:\n",
    "    raw_text = ifile.read()\n",
    "    \n",
    "#raw_text = \"Sample text to try functions\"    \n",
    "text_tokenized = removePunctuationFromTokenized(nltk.word_tokenize(raw_text))\n",
    "lower,upper = percentage_uppercased(text_tokenized)\n",
    "print(\"Lexical diversity: \" + \"{0:.3f}\".format(lexical_diversity(text_tokenized)))\n",
    "print(\"Lowercase sentences: \"+ \"{0:.3f}\".format(lower))\n",
    "print(\"Uppercase sentences: \"+ \"{0:.3f}\".format(upper))\n",
    "print(\"Spelling mistakes 1: \"  + str(spell_checker_1(text_tokenized)))\n",
    "print(\"Spelling mistakes 2: \"  + str(spell_checker_2(raw_text,text_tokenized)))\n",
    "print(\"Unrecognized by POS tagger: \"+str(unrecognized_by_pos(text_tokenized)))\n",
    "print(\"Average sentence length: \" +\"{0:.3f}\".format(avg_sentence_length(raw_text)))\n",
    "print(\"Readability (CLI): \" + str(textstat.coleman_liau_index(raw_text)))\n",
    "print(\"Readability (ARI): \" + str(textstat.automated_readability_index(raw_text)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20cb975c02109accbafa2be35d047791792b4e263bc20b6c63105d80068546e3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tesi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

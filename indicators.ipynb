{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef5d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "import enchant\n",
    "from spello.model import SpellCorrectionModel\n",
    "import textstat\n",
    "import re\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load spell checker model\n",
    "sp = SpellCorrectionModel(language='en')\n",
    "sp.load('./spello_model/en.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42370b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuationFromTokenized(contentsTokenized):\n",
    "    excludePuncuation = set(string.punctuation)\n",
    "    \n",
    "    # manually add additional punctuation to remove\n",
    "    doubleSingleQuote = '\\'\\''\n",
    "    doubleDash = '--'\n",
    "    doubleTick = '``'\n",
    "\n",
    "    excludePuncuation.add(doubleSingleQuote)\n",
    "    excludePuncuation.add(doubleDash)\n",
    "    excludePuncuation.add(doubleTick)\n",
    "\n",
    "    filteredContents = [word for word in contentsTokenized if word not in excludePuncuation]\n",
    "    return filteredContents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae9d0e6",
   "metadata": {},
   "source": [
    "#### Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881834bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "   return (len(set(text)) / len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14926723",
   "metadata": {},
   "source": [
    "#### Percentage of Uppercased and Lowercased Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_uppercased(text_tokenized):\n",
    "    islower = 0;\n",
    "    isupper = 0;\n",
    "    total_words = len(text_tokenized);\n",
    "\n",
    "    for word in text_tokenized:      \n",
    "        if word.islower():\n",
    "            islower +=1         \n",
    "        elif word.isupper():\n",
    "            isupper+=1         \n",
    "\n",
    "    return (islower/total_words,isupper/total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bfec8",
   "metadata": {},
   "source": [
    "#### Spell Checking Against Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe900f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_checker_1(text_tokenized):\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    mistakes = 0\n",
    "    for word in text_tokenized:\n",
    "        if not d.check(word):\n",
    "            mistakes+=1\n",
    "    return mistakes        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df6df67",
   "metadata": {},
   "source": [
    "#### Spell Checking with ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986728bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_checker_2(raw_text,text_tokenized):\n",
    "    corrected = sp.spell_correct(raw_text)\n",
    "    i = 0\n",
    "    for w in text_tokenized:\n",
    "        if(w in corrected['correction_dict']):\n",
    "            i+=1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d6848",
   "metadata": {},
   "source": [
    "#### Unrecognized Words by POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unrecognized_by_pos(text_tokenized):\n",
    "    text_tagged = nltk.pos_tag(text_tokenized,tagset='universal')\n",
    "    unknown = 0\n",
    "    for t in text_tagged:\n",
    "         if t[1] == \"X\":\n",
    "                #return t\n",
    "                unknown += 1\n",
    "    return unknown/len(text_tagged)            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c628c",
   "metadata": {},
   "source": [
    "#### Average Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8752ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_length(text):\n",
    "    \n",
    "    terminating_punct = \"[!?.]\"\n",
    "    punct = r\"\\W\"  # non-word characters\n",
    "    sentences = [\n",
    "        s.strip()  # without trailing whitespace\n",
    "        for s in re.split(\n",
    "            terminating_punct,\n",
    "            \"\".join(text).replace(\"\\n\", \" \"),  # text as 1 string\n",
    "        )\n",
    "        if s.strip()  # non-empty\n",
    "    ]\n",
    "\n",
    "    def wordcount(s):\n",
    "        \"\"\"Split sentence s on punctuation\n",
    "        and return number of non-empty words\n",
    "        \"\"\"\n",
    "        return len([w for w in re.split(punct, s) if w])\n",
    "    #map each sentece to its wordcount then sum all the wordcounts\n",
    "    return sum(map(wordcount, sentences)) / len(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6542d5",
   "metadata": {},
   "source": [
    "#### Acronyms 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d71047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this finds acronyms with with uppercase charcters with numbers inside\n",
    "def acronym1(s):\n",
    "    return re.findall(r\"\\b(?:[0-9]+[A-Z][A-Z0-9]*)|(?:[A-Z][A-Z0-9]+)\\b\", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6542d5",
   "metadata": {},
   "source": [
    "#### Acronyms 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this finds acronyms of all letters in uppercase and all letters in uppercase followed by dots \n",
    "def acronym2(s):\n",
    "    return re.findall(r\"\\b[A-Z\\.]{2,}\\b\", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6542d5",
   "metadata": {},
   "source": [
    "#### Acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acronym(s):\n",
    "    acronyms = re.findall(r\"\\b(?:[0-9]+[A-Z][A-Z0-9]*)|(?:[A-Z][A-Z0-9]+)\\b|\\b[A-Z\\.]{2,}\\b\", s)\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    for acronym in acronyms:\n",
    "        if d.check(acronym.lower()):\n",
    "            acronyms.remove(acronym)\n",
    "        \n",
    "    return  acronyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/corpora/acronimi2.txt\"\n",
    "\n",
    "with open(path, \"r\") as ifile:\n",
    "    raw_text = ifile.read()\n",
    "    \n",
    "#raw_text = \"Sample text to try functions\"    \n",
    "text_tokenized = removePunctuationFromTokenized(nltk.word_tokenize(raw_text))\n",
    "lower,upper = percentage_uppercased(text_tokenized)\n",
    "print(\"Lexical diversity: \" + \"{0:.3f}\".format(lexical_diversity(text_tokenized)))\n",
    "print(\"Lowercase sentences: \"+ \"{0:.3f}\".format(lower))\n",
    "print(\"Uppercase sentences: \"+ \"{0:.3f}\".format(upper))\n",
    "print(\"Spelling mistakes 1: \"  + str(spell_checker_1(text_tokenized)))\n",
    "print(\"Spelling mistakes 2: \"  + str(spell_checker_2(raw_text,text_tokenized)))\n",
    "print(\"Unrecognized by POS tagger: \"+str(unrecognized_by_pos(text_tokenized)))\n",
    "print(\"Average sentence length: \" +\"{0:.3f}\".format(avg_sentence_length(raw_text)))\n",
    "print(\"Readability (CLI): \" + str(textstat.coleman_liau_index(raw_text)))\n",
    "print(\"Readability (ARI): \" + str(textstat.automated_readability_index(raw_text)))\n",
    "print(\"Acronyms:\" + str(acronym(raw_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8109e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clsuters:-\n",
      "[[0, 1], [5, 5]]\n",
      "\n",
      "\n",
      "\n",
      "Coref resolved:  Joe Biden is president. Joe Biden's healty.\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "#model_url = \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\"\n",
    "#predictor = Predictor.from_path(model_url)\n",
    "\n",
    "text = \"Joe Biden is president. He's healty.\"\n",
    "\n",
    "\n",
    "prediction = predictor.predict(document=text)  # get prediction\n",
    "print(\"Clsuters:-\")\n",
    "for cluster in prediction['clusters']:\n",
    "    print(cluster)  # list of clusters (the indices of spaCy tokens)\n",
    "# Result: [[[0, 3], [26, 26]], [[34, 34], [50, 50]]]\n",
    "print('\\n\\n') #Newline\n",
    "\n",
    "print('Coref resolved: ',predictor.coref_resolved(text))  # resolved text\n",
    "# Result: Joseph Robinette Biden Jr. is an American politician who is the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03496969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20cb975c02109accbafa2be35d047791792b4e263bc20b6c63105d80068546e3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tesi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

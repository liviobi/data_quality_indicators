{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"uploadDir\": \"./data\",\n",
    "    \"optimal_sentence_length\": 16,\n",
    "}\n",
    "\n",
    "indicatorsTemplate = {\n",
    "    \"parsable\": None,\n",
    "    \"confidence_tokenizer\": None,\n",
    "    \"confidence_pos\": None,\n",
    "    \"confidence_ner\": None,\n",
    "    \"confidence_chunker\": None,\n",
    "    \"fit\": None,\n",
    "    \"spelling_mistakes\": None,\n",
    "    \"avg_sentence_len\": None,\n",
    "    \"perc_lowercase\": None,\n",
    "    \"perc_uppercase\": None,\n",
    "    \"lexical_diversity\": None,\n",
    "    \"recognized_by_pos\": None,\n",
    "    \"acronyms\": None,\n",
    "    \"present_in_dictionary\": None,\n",
    "    \"readability_cli\": None,\n",
    "    \"readability_ari\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from http import server\n",
    "import copy\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "from subprocess import check_output\n",
    "from threading import Thread\n",
    "import time\n",
    "import string\n",
    "import nltk\n",
    "import enchant\n",
    "from spello.model import SpellCorrectionModel\n",
    "import re\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lbiondo\\Anaconda3\\envs\\tesi\\lib\\site-packages\\spello\\model.py:301: UserWarning: This model was saved on spell<1.3.0. As such due to a bug in previous versions, none of customisations made to the config at the time of training were saved along with the model. It is recommended to load the model, apply all required customizations to config and save it again. E.g.\n",
      "\n",
      "from spello.model import SpellCorrectionModel \n",
      "sp = SpellCorrectionModel(language='en')  \n",
      "sp.load('/home/ubuntu/model.pkl')\n",
      "sp.config.min_length_for_spellcorrection = 4 # default is 3\n",
      "sp.config.max_length_for_spellcorrection = 12 # default is 15\n",
      "sp.save(model_save_dir='/home/ubuntu/')\n",
      "\n",
      "After this the model will load without any warnings\n",
      "\n",
      "  warnings.warn(\"This model was saved on spell<1.3.0. As such due to a bug in previous versions, \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spello.model.SpellCorrectionModel at 0x18e560fa6e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = SpellCorrectionModel(language='en')\n",
    "# sp.load('./spello_model/en_large.pkl')\n",
    "sp.load('./spello_model/en.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list where I'm going to save the indicators for each filename\n",
    "files = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def removePunctuationFromTokenized(contentsTokenized):\n",
    "    excludePuncuation = set(string.punctuation)\n",
    "\n",
    "    # manually add additional punctuation to remove\n",
    "    doubleSingleQuote = '\\'\\''\n",
    "    doubleDash = '--'\n",
    "    doubleTick = '``'\n",
    "\n",
    "    excludePuncuation.add(doubleSingleQuote)\n",
    "    excludePuncuation.add(doubleDash)\n",
    "    excludePuncuation.add(doubleTick)\n",
    "\n",
    "    filteredContents = [\n",
    "        word for word in contentsTokenized if word not in excludePuncuation]\n",
    "    return filteredContents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSpellingMistakes(filename, indicator):\n",
    "    print(f\"running computation of {indicator} for {filename}\")\n",
    "    with open( os.path.join(cfg[\"uploadDir\"], filename), \"r\") as f:\n",
    "        raw_text = f.read()\n",
    "        text_tokenized = removePunctuationFromTokenized(\n",
    "            nltk.word_tokenize(raw_text))\n",
    "        corrected = sp.spell_correct(raw_text)\n",
    "        mistakes = 0\n",
    "        for w in text_tokenized:\n",
    "            if(w in corrected['correction_dict']):\n",
    "                mistakes += 1\n",
    "        result = (1 - (mistakes / len(text_tokenized)))*100\n",
    "        files[filename][indicator] = str(result)[0:4]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeRecognizedByPOS(filename, indicator):\n",
    "    print(f\"running computation of {indicator} for {filename}\")\n",
    "    with open(os.path.join(cfg[\"uploadDir\"], filename), \"r\") as f:\n",
    "        raw_text = f.read()\n",
    "        text_tokenized = removePunctuationFromTokenized(\n",
    "            nltk.word_tokenize(raw_text))\n",
    "\n",
    "        text_tagged = nltk.pos_tag(text_tokenized, tagset='universal')\n",
    "        unknown = 0\n",
    "        for t in text_tagged:\n",
    "            if t[1] == \"X\":\n",
    "                unknown += 1\n",
    "        result = (1 - (unknown/len(text_tagged)))*100\n",
    "        files[filename][indicator] = str(result)[0:4]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcount(s):\n",
    "    \"\"\"Split sentence s on punctuation\n",
    "    and return number of non-empty words\n",
    "    \"\"\"\n",
    "    punct = r\"\\W\"  # non-word characters\n",
    "    return len([w for w in re.split(punct, s) if w])\n",
    "\n",
    "def computeAvgSentLen(filename, indicator):\n",
    "    print(f\"running computation of {indicator} for {filename}\")\n",
    "    with open(os.path.join(cfg[\"uploadDir\"], filename), \"r\") as f:\n",
    "        raw_text = f.read()\n",
    "        terminating_punct = \"[!?.]\"\n",
    "        sentences = [\n",
    "            s.strip()  # without trailing whitespace\n",
    "            for s in re.split(\n",
    "                terminating_punct,\n",
    "                \"\".join(raw_text).replace(\"\\n\", \" \"),  # text as 1 string\n",
    "            )\n",
    "            if s.strip()  # non-empty\n",
    "        ]\n",
    "        # map each sentece to its wordcount then sum all the wordcounts\n",
    "        avgSentenceLength = sum(map(wordcount, sentences)) / len(sentences)\n",
    "        optimalSentenceLen = cfg[\"optimal_sentence_length\"]\n",
    "        if avgSentenceLength > 2*optimalSentenceLen:\n",
    "            avgSentenceLength = 2*optimalSentenceLen\n",
    "        result = (1 - abs(optimalSentenceLen - avgSentenceLength) /\n",
    "                  optimalSentenceLen) * 100\n",
    "        files[filename][indicator] = str(result)[0:4]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePresentInDictionary(filename, indicator):\n",
    "    print(f\"running computation of {indicator} for {filename}\")\n",
    "    with open(os.path.join(cfg[\"uploadDir\"], filename), \"r\") as f:\n",
    "        raw_text = f.read()\n",
    "        text_tokenized = removePunctuationFromTokenized(\n",
    "            nltk.word_tokenize(raw_text))\n",
    "\n",
    "        d = enchant.Dict(\"en_US\")\n",
    "        correct = 0\n",
    "        for word in text_tokenized:\n",
    "            if d.check(word):\n",
    "                correct += 1\n",
    "        result = (correct / len(text_tokenized))*100\n",
    "        files[filename][indicator] = str(result)[0:4]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLexicalDiversity(filename, indicator):\n",
    "    print(f\"running computation of {indicator} for {filename}\")\n",
    "    with open(os.path.join(cfg[\"uploadDir\"], filename), \"r\") as f:\n",
    "        raw_text = f.read()\n",
    "        text_tokenized = removePunctuationFromTokenized(\n",
    "            nltk.word_tokenize(raw_text))\n",
    "\n",
    "        # TODO normalize\n",
    "\n",
    "        result = (len(set(text_tokenized)) / len(text_tokenized))*100\n",
    "        files[filename][indicator] = str(result)[0:4]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setJavaIndicators(filename, result):\n",
    "    files[filename][\"parsable\"] = result[0][0:4]\n",
    "    files[filename][\"confidence_tokenizer\"] = result[1][0:4]\n",
    "    files[filename][\"confidence_pos\"] = result[2][0:4]\n",
    "    files[filename][\"confidence_ner\"] = result[3][0:4]\n",
    "    files[filename][\"confidence_chunker\"] = result[4][0:4]\n",
    "\n",
    "def computeJavaIndicators(filename):\n",
    "    # get the absolute path of the file to pass as argument to jar\n",
    "    path = os.path.abspath(os.path.join(cfg[\"uploadDir\"], filename))\n",
    "    pathModels = os.path.abspath(\"./java-indicators/models\")\n",
    "    # launch java jar\n",
    "    result = check_output(\n",
    "        ['java', '-jar', './java-indicators/java-indicators.jar', path, pathModels])\n",
    "    setJavaIndicators(filename, result.decode().split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAcronyms(filename, indicator):\n",
    "    with open(os.path.join(cfg[\"uploadDir\"], filename), \"r\") as f:\n",
    "        raw_text = f.read()\n",
    "        text_tokenized = removePunctuationFromTokenized(\n",
    "            nltk.word_tokenize(raw_text))\n",
    "        acronym_list = re.findall(\n",
    "            r\"\\b(?:[0-9]+[A-Z][A-Z0-9]*)|(?:[A-Z][A-Z0-9]+)\\b|\\b[A-Z\\.]{2,}\\b\", raw_text)\n",
    "        acronyms_count = 0\n",
    "        for word in text_tokenized:\n",
    "            if word in acronym_list:\n",
    "                acronyms_count += 1\n",
    "        result = (1-(acronyms_count / len(text_tokenized)))*100\n",
    "        files[filename][indicator] = str(result)[0:4]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeReadabilityCli( filename, indicator):\n",
    "    print(f\"running computation of {indicator} for {filename}\")\n",
    "    with open(os.path.join(cfg[\"uploadDir\"], filename), \"r\") as f:\n",
    "        raw_text = f.read()\n",
    "        score = textstat.coleman_liau_index(raw_text)\n",
    "        optimalScore = 3\n",
    "        worstScore = 18\n",
    "\n",
    "        if(score > worstScore):\n",
    "            score = worstScore\n",
    "\n",
    "        result = (1 - abs(optimalScore - score) /\n",
    "                  (worstScore - optimalScore)) * 100\n",
    "        files[filename][indicator] = str(result)[0:4]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeReadabilityAri(filename, indicator):\n",
    "    print(f\"running computation of {indicator} for {filename}\")\n",
    "    with open(os.path.join(cfg[\"uploadDir\"], filename), \"r\") as f:\n",
    "        raw_text = f.read()\n",
    "        score = textstat.automated_readability_index(raw_text)\n",
    "        optimalScore = 3\n",
    "        worstScore = 18\n",
    "\n",
    "        if(score > worstScore):\n",
    "            score = worstScore\n",
    "\n",
    "        result = (1 - abs(optimalScore - score) /\n",
    "                  (worstScore - optimalScore)) * 100\n",
    "        files[filename][indicator] = str(result)[0:4]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running computation of spelling_mistakes for acronimi2.txt\n",
      "running computation of avg_sentence_len for acronimi2.txt\n",
      "running computation of lexical_diversity for acronimi2.txt\n",
      "running computation of recognized_by_pos for acronimi2.txt\n",
      "running computation of present_in_dictionary for acronimi2.txt\n",
      "running computation of readability_cli for acronimi2.txt\n",
      "running computation of readability_ari for acronimi2.txt\n",
      "running computation of spelling_mistakes for prose.txt\n",
      "running computation of avg_sentence_len for prose.txt\n",
      "running computation of lexical_diversity for prose.txt\n",
      "running computation of recognized_by_pos for prose.txt\n",
      "running computation of present_in_dictionary for prose.txt\n",
      "running computation of readability_cli for prose.txt\n",
      "running computation of readability_ari for prose.txt\n"
     ]
    }
   ],
   "source": [
    "listOfFileNames = [fileName for fileName in listdir(cfg[\"uploadDir\"]) if isfile(join(cfg[\"uploadDir\"], fileName))]\n",
    "\n",
    "for filename in listOfFileNames:\n",
    "    #populate the dictionary\n",
    "    files[filename] = copy.deepcopy(indicatorsTemplate)\n",
    "    computeJavaIndicators(filename)\n",
    "    computeSpellingMistakes(filename,\"spelling_mistakes\")\n",
    "    computeAvgSentLen(filename,\"avg_sentence_len\")\n",
    "    computeLexicalDiversity(filename,\"lexical_diversity\")\n",
    "    computeRecognizedByPOS(filename, \"recognized_by_pos\")\n",
    "    computeAcronyms(filename, \"acronyms\")\n",
    "    computePresentInDictionary(filename,\"present_in_dictionary\")\n",
    "    computeReadabilityCli(filename,\"readability_cli\")\n",
    "    computeReadabilityAri(filename,\"readability_ari\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfFileNames = [fileName for fileName in listdir(cfg[\"uploadDir\"]) if isfile(join(cfg[\"uploadDir\"], fileName))]\n",
    "\n",
    "indicatorsList = []\n",
    "\n",
    "for f in listOfFileNames:\n",
    "    indicatorsList.append(list(files[f].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parsable</th>\n",
       "      <th>confidence_tokenizer</th>\n",
       "      <th>confidence_pos</th>\n",
       "      <th>confidence_ner</th>\n",
       "      <th>confidence_chunker</th>\n",
       "      <th>fit</th>\n",
       "      <th>spelling_mistakes</th>\n",
       "      <th>avg_sentence_len</th>\n",
       "      <th>perc_lowercase</th>\n",
       "      <th>perc_uppercase</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>recognized_by_pos</th>\n",
       "      <th>acronyms</th>\n",
       "      <th>present_in_dictionary</th>\n",
       "      <th>readability_cli</th>\n",
       "      <th>readability_ari</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>98.4</td>\n",
       "      <td>64.8</td>\n",
       "      <td>97.7</td>\n",
       "      <td>80.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.8</td>\n",
       "      <td>37.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.6</td>\n",
       "      <td>95.6</td>\n",
       "      <td>86.9</td>\n",
       "      <td>39.1</td>\n",
       "      <td>64.8</td>\n",
       "      <td>71.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.0</td>\n",
       "      <td>98.6</td>\n",
       "      <td>92.0</td>\n",
       "      <td>99.4</td>\n",
       "      <td>94.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.0</td>\n",
       "      <td>98.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.6</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98.3</td>\n",
       "      <td>65.8</td>\n",
       "      <td>41.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parsable  confidence_tokenizer  confidence_pos  confidence_ner  \\\n",
       "0     100.0                  98.4            64.8            97.7   \n",
       "1     100.0                  98.6            92.0            99.4   \n",
       "\n",
       "   confidence_chunker  fit  spelling_mistakes  avg_sentence_len  \\\n",
       "0                80.7  NaN               60.8              37.5   \n",
       "1                94.2  NaN               97.0              98.9   \n",
       "\n",
       "   perc_lowercase  perc_uppercase  lexical_diversity  recognized_by_pos  \\\n",
       "0             NaN             NaN               95.6               95.6   \n",
       "1             NaN             NaN               42.6              100.0   \n",
       "\n",
       "   acronyms  present_in_dictionary  readability_cli  readability_ari  \n",
       "0      86.9                   39.1             64.8             71.3  \n",
       "1     100.0                   98.3             65.8             41.3  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ind_df = pd.DataFrame(indicatorsList, columns=list(indicatorsTemplate.keys()))\n",
    "ind_df.replace({'100.': '100'}, regex=True, inplace=True)\n",
    "ind_df = ind_df.astype(float)\n",
    "ind_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the 100. strings\n",
    "ind_df.replace({'100.': '100'}, regex=True, inplace=True)\n",
    "ind_df = ind_df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>parsable</th>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confidence_tokenizer</th>\n",
       "      <td>98.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confidence_pos</th>\n",
       "      <td>78.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confidence_ner</th>\n",
       "      <td>98.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confidence_chunker</th>\n",
       "      <td>87.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spelling_mistakes</th>\n",
       "      <td>78.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_sentence_len</th>\n",
       "      <td>68.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perc_lowercase</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perc_uppercase</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexical_diversity</th>\n",
       "      <td>69.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recognized_by_pos</th>\n",
       "      <td>97.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acronyms</th>\n",
       "      <td>93.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>present_in_dictionary</th>\n",
       "      <td>68.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>readability_cli</th>\n",
       "      <td>65.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>readability_ari</th>\n",
       "      <td>56.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0\n",
       "parsable               100.00\n",
       "confidence_tokenizer    98.50\n",
       "confidence_pos          78.40\n",
       "confidence_ner          98.55\n",
       "confidence_chunker      87.45\n",
       "fit                       NaN\n",
       "spelling_mistakes       78.90\n",
       "avg_sentence_len        68.20\n",
       "perc_lowercase            NaN\n",
       "perc_uppercase            NaN\n",
       "lexical_diversity       69.10\n",
       "recognized_by_pos       97.80\n",
       "acronyms                93.45\n",
       "present_in_dictionary   68.70\n",
       "readability_cli         65.30\n",
       "readability_ari         56.30"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average of each column using DataFrame.mean()\n",
    "df2 = ind_df.mean(axis=0).to_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>parsable</th>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confidence_tokenizer</th>\n",
       "      <td>98.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confidence_pos</th>\n",
       "      <td>78.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confidence_ner</th>\n",
       "      <td>98.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confidence_chunker</th>\n",
       "      <td>87.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spelling_mistakes</th>\n",
       "      <td>78.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_sentence_len</th>\n",
       "      <td>68.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexical_diversity</th>\n",
       "      <td>69.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recognized_by_pos</th>\n",
       "      <td>97.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acronyms</th>\n",
       "      <td>93.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>present_in_dictionary</th>\n",
       "      <td>68.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>readability_cli</th>\n",
       "      <td>65.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>readability_ari</th>\n",
       "      <td>56.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0\n",
       "parsable               100.00\n",
       "confidence_tokenizer    98.50\n",
       "confidence_pos          78.40\n",
       "confidence_ner          98.55\n",
       "confidence_chunker      87.45\n",
       "spelling_mistakes       78.90\n",
       "avg_sentence_len        68.20\n",
       "lexical_diversity       69.10\n",
       "recognized_by_pos       97.80\n",
       "acronyms                93.45\n",
       "present_in_dictionary   68.70\n",
       "readability_cli         65.30\n",
       "readability_ari         56.30"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20cb975c02109accbafa2be35d047791792b4e263bc20b6c63105d80068546e3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tesi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

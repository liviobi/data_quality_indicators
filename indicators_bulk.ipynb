{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"uploadDir\": \"./data/corpora/reuters_merged\",\n",
    "    \"resultDir\": \"./data/results\",\n",
    "    \"optimal_sentence_length\": 16,\n",
    "}\n",
    "\n",
    "indicatorsTemplate = {\n",
    "    \"parsable\": None,\n",
    "    \"confidence_tokenizer\": None,\n",
    "    \"confidence_pos\": None,\n",
    "    \"confidence_ner\": None,\n",
    "    \"confidence_chunker\": None,\n",
    "    \"fit\": None,\n",
    "    \"spelling_mistakes\": None,\n",
    "    \"avg_sentence_len\": None,\n",
    "    \"perc_lowercase\": None,\n",
    "    \"perc_uppercase\": None,\n",
    "    \"lexical_diversity\": None,\n",
    "    \"recognized_by_pos\": None,\n",
    "    \"acronyms\": None,\n",
    "    \"present_in_dictionary\": None,\n",
    "    \"readability_cli\": None,\n",
    "    \"readability_ari\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from http import server\n",
    "import copy\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "from subprocess import check_output\n",
    "from threading import Thread\n",
    "import time\n",
    "import string\n",
    "import nltk\n",
    "import enchant\n",
    "from spello.model import SpellCorrectionModel\n",
    "import re\n",
    "import textstat\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SpellCorrectionModel(language='en')\n",
    "# sp.load('./spello_model/en_large.pkl')\n",
    "sp.load('./spello_model/en_large.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list where I'm going to save the indicators for each filename\n",
    "files = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def removePunctuationFromTokenized(contentsTokenized):\n",
    "    excludePuncuation = set(string.punctuation)\n",
    "\n",
    "    # manually add additional punctuation to remove\n",
    "    doubleSingleQuote = '\\'\\''\n",
    "    doubleDash = '--'\n",
    "    doubleTick = '``'\n",
    "\n",
    "    excludePuncuation.add(doubleSingleQuote)\n",
    "    excludePuncuation.add(doubleDash)\n",
    "    excludePuncuation.add(doubleTick)\n",
    "\n",
    "    filteredContents = [\n",
    "        word for word in contentsTokenized if word not in excludePuncuation]\n",
    "    return filteredContents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSpellingMistakes(filename, indicator):\n",
    "    with open( os.path.join(cfg[\"uploadDir\"], filename), \"r\") as f:\n",
    "        raw_text = f.read()\n",
    "        text_tokenized = removePunctuationFromTokenized(\n",
    "            nltk.word_tokenize(raw_text))\n",
    "        corrected = sp.spell_correct(raw_text)\n",
    "        mistakes = 0\n",
    "        for w in text_tokenized:\n",
    "            if(w in corrected['correction_dict']):\n",
    "                mistakes += 1\n",
    "        result = (1 - (mistakes / len(text_tokenized)))*100\n",
    "        files[filename][indicator] = str(result)[0:4]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeRecognizedByPOS(filename, indicator):\n",
    "    with open(os.path.join(cfg[\"uploadDir\"], filename), \"r\") as f:\n",
    "        raw_text = f.read()\n",
    "        text_tokenized = removePunctuationFromTokenized(\n",
    "            nltk.word_tokenize(raw_text))\n",
    "\n",
    "        text_tagged = nltk.pos_tag(text_tokenized, tagset='universal')\n",
    "        unknown = 0\n",
    "        for t in text_tagged:\n",
    "            if t[1] == \"X\":\n",
    "                unknown += 1\n",
    "        result = (1 - (unknown/len(text_tagged)))*100\n",
    "        files[filename][indicator] = str(result)[0:4]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcount(s):\n",
    "    \"\"\"Split sentence s on punctuation\n",
    "    and return number of non-empty words\n",
    "    \"\"\"\n",
    "    punct = r\"\\W\"  # non-word characters\n",
    "    return len([w for w in re.split(punct, s) if w])\n",
    "\n",
    "def computeAvgSentLen(filename, indicator):\n",
    "    with open(os.path.join(cfg[\"uploadDir\"], filename), \"r\") as f:\n",
    "        raw_text = f.read()\n",
    "        terminating_punct = \"[!?.]\"\n",
    "        sentences = [\n",
    "            s.strip()  # without trailing whitespace\n",
    "            for s in re.split(\n",
    "                terminating_punct,\n",
    "                \"\".join(raw_text).replace(\"\\n\", \" \"),  # text as 1 string\n",
    "            )\n",
    "            if s.strip()  # non-empty\n",
    "        ]\n",
    "        # map each sentece to its wordcount then sum all the wordcounts\n",
    "        avgSentenceLength = sum(map(wordcount, sentences)) / len(sentences)\n",
    "        optimalSentenceLen = cfg[\"optimal_sentence_length\"]\n",
    "        if avgSentenceLength > 2*optimalSentenceLen:\n",
    "            avgSentenceLength = 2*optimalSentenceLen\n",
    "        result = (1 - abs(optimalSentenceLen - avgSentenceLength) /\n",
    "                  optimalSentenceLen) * 100\n",
    "        files[filename][indicator] = str(result)[0:4]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePresentInDictionary(filename, indicator):\n",
    "    with open(os.path.join(cfg[\"uploadDir\"], filename), \"r\") as f:\n",
    "        raw_text = f.read()\n",
    "        text_tokenized = removePunctuationFromTokenized(\n",
    "            nltk.word_tokenize(raw_text))\n",
    "\n",
    "        d = enchant.Dict(\"en_US\")\n",
    "        correct = 0\n",
    "        for word in text_tokenized:\n",
    "            if d.check(word):\n",
    "                correct += 1\n",
    "        result = (correct / len(text_tokenized))*100\n",
    "        files[filename][indicator] = str(result)[0:4]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLexicalDiversity(filename, indicator):\n",
    "    with open(os.path.join(cfg[\"uploadDir\"], filename), \"r\") as f:\n",
    "        raw_text = f.read()\n",
    "        text_tokenized = removePunctuationFromTokenized(\n",
    "            nltk.word_tokenize(raw_text))\n",
    "\n",
    "        # TODO normalize\n",
    "\n",
    "        result = (len(set(text_tokenized)) / len(text_tokenized))*100\n",
    "        files[filename][indicator] = str(result)[0:4]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setJavaIndicators(filename, result):\n",
    "    files[filename][\"parsable\"] = result[0][0:4]\n",
    "    files[filename][\"confidence_tokenizer\"] = result[1][0:4]\n",
    "    files[filename][\"confidence_pos\"] = result[2][0:4]\n",
    "    files[filename][\"confidence_ner\"] = result[3][0:4]\n",
    "    files[filename][\"confidence_chunker\"] = result[4][0:4]\n",
    "\n",
    "def computeJavaIndicators(filename):\n",
    "    # get the absolute path of the file to pass as argument to jar\n",
    "    path = os.path.abspath(os.path.join(cfg[\"uploadDir\"], filename))\n",
    "    pathModels = os.path.abspath(\"./java-indicators/models\")\n",
    "    # launch java jar\n",
    "    result = check_output(\n",
    "        ['java', '-jar', './java-indicators/java-indicators.jar', path, pathModels])\n",
    "    setJavaIndicators(filename, result.decode().split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAcronyms(filename, indicator):\n",
    "    with open(os.path.join(cfg[\"uploadDir\"], filename), \"r\") as f:\n",
    "        raw_text = f.read()\n",
    "        text_tokenized = removePunctuationFromTokenized(\n",
    "            nltk.word_tokenize(raw_text))\n",
    "        acronym_list = re.findall(\n",
    "            r\"\\b(?:[0-9]+[A-Z][A-Z0-9]*)|(?:[A-Z][A-Z0-9]+)\\b|\\b[A-Z\\.]{2,}\\b\", raw_text)\n",
    "        \n",
    "        #remove upper case words present in dictionary from the list of acronyms\n",
    "        d = enchant.Dict(\"en_US\")\n",
    "        for acronym in acronym_list:\n",
    "            if d.check(acronym.lower()):\n",
    "                acronym_list.remove(acronym)\n",
    "        \n",
    "        acronyms_count = 0\n",
    "        for word in text_tokenized:\n",
    "            if word in acronym_list:\n",
    "                acronyms_count += 1\n",
    "        result = (1-(acronyms_count / len(text_tokenized)))*100\n",
    "        files[filename][indicator] = str(result)[0:4]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeReadabilityCli( filename, indicator):\n",
    "    with open(os.path.join(cfg[\"uploadDir\"], filename), \"r\") as f:\n",
    "        raw_text = f.read()\n",
    "        score = textstat.coleman_liau_index(raw_text)\n",
    "        optimalScore = 3\n",
    "        worstScore = 18\n",
    "\n",
    "        if(score > worstScore):\n",
    "            score = worstScore\n",
    "\n",
    "        result = (1 - abs(optimalScore - score) /\n",
    "                  (worstScore - optimalScore)) * 100\n",
    "        files[filename][indicator] = str(result)[0:4]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeReadabilityAri(filename, indicator):\n",
    "    with open(os.path.join(cfg[\"uploadDir\"], filename), \"r\") as f:\n",
    "        raw_text = f.read()\n",
    "        score = textstat.automated_readability_index(raw_text)\n",
    "        optimalScore = 3\n",
    "        worstScore = 18\n",
    "\n",
    "        if(score > worstScore):\n",
    "            score = worstScore\n",
    "\n",
    "        result = (1 - abs(optimalScore - score) /\n",
    "                  (worstScore - optimalScore)) * 100\n",
    "        files[filename][indicator] = str(result)[0:4]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "listOfFileNames = [fileName for fileName in listdir(cfg[\"uploadDir\"]) if isfile(join(cfg[\"uploadDir\"], fileName))]\n",
    "\n",
    "i = 1\n",
    "totalFiles = len(listOfFileNames)\n",
    "for filename in listOfFileNames:\n",
    "    clear_output(wait=False)\n",
    "    print(f\"Analyzing {filename} | {str(i)} of {totalFiles}\")\n",
    "    i += 1\n",
    "    #populate the dictionary\n",
    "    files[filename] = copy.deepcopy(indicatorsTemplate)\n",
    "    computeJavaIndicators(filename)\n",
    "    computeSpellingMistakes(filename,\"spelling_mistakes\")\n",
    "    computeAvgSentLen(filename,\"avg_sentence_len\")\n",
    "    computeLexicalDiversity(filename,\"lexical_diversity\")\n",
    "    computeRecognizedByPOS(filename, \"recognized_by_pos\")\n",
    "    computeAcronyms(filename, \"acronyms\")\n",
    "    computePresentInDictionary(filename,\"present_in_dictionary\")\n",
    "    computeReadabilityCli(filename,\"readability_cli\")\n",
    "    computeReadabilityAri(filename,\"readability_ari\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfFileNames = [fileName for fileName in listdir(cfg[\"uploadDir\"]) if isfile(join(cfg[\"uploadDir\"], fileName))]\n",
    "\n",
    "indicatorsList = []\n",
    "\n",
    "for f in listOfFileNames:\n",
    "    indicatorsList.append(list(files[f].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_df = pd.DataFrame(indicatorsList, columns=list(indicatorsTemplate.keys()))\n",
    "ind_df.replace({'100.': '100'}, regex=True, inplace=True)\n",
    "ind_df = ind_df.astype(float)\n",
    "ind_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the 100. strings\n",
    "ind_df.replace({'100.': '100'}, regex=True, inplace=True)\n",
    "ind_df = ind_df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average of each column using DataFrame.mean()\n",
    "df2 = ind_df.mean(axis=0).to_frame()\n",
    "df2 = df2.dropna()\n",
    "df2 = df2.T\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputCsvName = cfg[\"uploadDir\"].replace(\"./data/corpora/\",\"\") + \".csv\"\n",
    "outputPath = os.path.join(cfg[\"resultDir\"],outputCsvName)\n",
    "df2.to_csv(outputPath, index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20cb975c02109accbafa2be35d047791792b4e263bc20b6c63105d80068546e3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tesi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
